.. This Software (Dioptra) is being made available as a public service by the
.. National Institute of Standards and Technology (NIST), an Agency of the United
.. States Department of Commerce. This software was developed in part by employees of
.. NIST and in part by NIST contractors. Copyright in portions of this software that
.. were developed by NIST contractors has been licensed or assigned to NIST. Pursuant
.. to Title 17 United States Code Section 105, works of NIST employees are not
.. subject to copyright protection in the United States. However, NIST may hold
.. international copyright in software created by its employees and domestic
.. copyright (or licensing rights) in portions of software that were assigned or
.. licensed to NIST. To the extent that NIST holds copyright in this software, it is
.. being made available under the Creative Commons Attribution 4.0 International
.. license (CC BY 4.0). The disclaimers of the CC BY 4.0 license apply to all parts
.. of the software developed or licensed by NIST.
..
.. ACCESS THE FULL CC BY 4.0 LICENSE HERE:
.. https://creativecommons.org/licenses/by/4.0/legalcode

.. _deployment-guide-single-machine:

Single Machine Deployment
=========================

.. include:: /_glossary_note.rst

We recommend the use of the ``docker compose`` tool to orchestrate Testbed deployments on a single machine.
The `tool's home page <https://docs.docker.com/compose/>`__ provides an excellent summary of what it is and what it does.
For convenience, we quote the summary below,

   Compose is a tool for defining and running multi-container Docker applications.
   With Compose, you use a :term:`YAML` file to configure your application’s services.
   Then, with a single command, you create and start all the services from your configuration.
   To learn more about all the features of Compose, see the `list of features <https://docs.docker.com/compose/#features>`__.

   Compose works in all environments: production, staging, development, testing, as well as CI workflows. You can learn more about each case in `Common Use Cases <https://docs.docker.com/compose/#common-use-cases>`__.

   Using Compose is basically a three-step process:

   #. Define your app's environment with a ``Dockerfile`` so it can be reproduced anywhere.

   #. Define the services that make up your app in ``docker-compose.yml`` so they can be run together in an isolated environment.

   #. Run ``docker compose up`` and the `Docker compose command <https://docs.docker.com/compose/cli-command/>`__ starts and runs your entire app.
      You can alternatively run ``docker-compose up`` using the docker-compose binary.

   A ``docker-compose.yml`` looks like this:

   .. code-block:: yaml

      version: "3.9"  # optional since v1.27.0
      services:
        web:
          build: .
          ports:
            - "5000:5000"
          volumes:
            - .:/code
            - logvolume01:/var/log
          links:
            - redis
        redis:
          image: redis
      volumes:
        logvolume01: {}

   For more information about the Compose file, see the `Compose file reference <https://docs.docker.com/compose/compose-file/>`__.

   -- `Overview of Docker Compose <https://docs.docker.com/compose/>`__

Sample Deployment
-----------------

The sample ``docker-compose.yml`` file contained in the dropdown menu below is a copy of our single-machine Testbed deployment, which was for an :term:`NVIDIA DGX` workstation with the following specifications,

CPU
   Intel Xeon Processor with 20 physical cores at 2.2GHz
GPU
   4 Nvidia Tesla V100 (Volta) GPUs
RAM
   12.5GB per physical core
NFS Mount
   2TB of shared storage, mount point is ``/nfs/1/datasets``
Local Storage
   2TB solid state drive
Operating System
   Ubuntu 18.04 LTS

.. dropdown:: Single Machine Deployment docker-compose.yml file

   .. include:: _docker-compose-single-machine.rst

For the rest of this guide, we will discuss how to adapt this file for use in your deployment environment.
You will need root access to the machine in order to proceed, run ``sudo -s`` to become root.

We store our deployment ``docker-compose.yml`` file at the path ``/etc/dioptra/lab-deployment/docker-compose.yml`` and set the folder permissions so that non-root users cannot access the file.

Create Application Folders on Host
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The data generated by the Minio, MLFlow Tracking, Redis, and :term:`REST` :term:`API` services will be mapped to the host machine's filesystem via bind mounts.
We placed these folders for our deployment under the directory ``/var/dioptra/lab-deployment``, which looks as follows:

.. code-block:: none

   /var/dioptra/lab-deployment
   ├── minio
   │   ├── mlflow-tracking
   │   ├── plugins
   │   └── workflow
   ├── mlflow-tracking
   │   └── mlflow-tracking.db
   ├── redis
   │   └── appendonly.aof
   └── restapi
       └── dioptra.db

For the purposes of this guide, we are going to re-use the same folder structure for our deployment.
We create them as follows:

.. code-block:: sh

   # Create folders
   mkdir -p /var/dioptra/lab-deployment/minio
   mkdir -p /var/dioptra/lab-deployment/mlflow-tracking
   mkdir -p /var/dioptra/lab-deployment/redis
   mkdir -p /var/dioptra/lab-deployment/restapi

Next, we need to set the folder permissions to match the user and group ids of the non-root user accounts used by most of these services.

.. code-block:: sh

   # Prevent access to deployment folders by non-root users
   chmod 0750 /var/dioptra

   # Configure folder owners for compatibility with Docker images
   chown -R 39000:100 /var/dioptra/lab-deployment/mlflow-tracking
   chown -R 39000:100 /var/dioptra/lab-deployment/restapi
   chown -R 39000:100 /var/dioptra/lab-deployment/minio
   chown -R 999:100 /var/dioptra/lab-deployment/redis

   # Configure folder permissions
   chmod 0700 /var/dioptra/lab-deployment/mlflow-tracking
   chmod 0700 /var/dioptra/lab-deployment/restapi
   chmod 0700 /var/dioptra/lab-deployment/minio
   chmod 0700 /var/dioptra/lab-deployment/redis

Parameters to Set/Update
~~~~~~~~~~~~~~~~~~~~~~~~

Now that we've set up our folder structure, there are a few changes that you will need to make to the ``docker-compose.yml`` file to prepare it for your environment.

.. rubric:: Configure the dataset volume

The ``nfs-datasets`` volume will need to be updated based on the details of your deployment,

.. tab-set::

   .. tab-item:: Datasets on NFS share

      The following Docker volume assumes that the NFS share is the directory ``/var/nfs/datasets`` on a device with the IP address ``192.168.1.20``.
      Update it to match the details of your NFS share.

      .. code-block:: yaml

         nfs-datasets:
           driver: local
           driver_opts:
             type: nfs
             o: addr=192.168.1.20,nfsvers=4,ro,soft,nolock,async,noatime,intr,tcp,rsize=131072,wsize=131072,actimeo=1800
             device: ":/var/nfs/datasets"

   .. tab-item:: Datasets in local directory

      The following Docker volume assumes that the data is stored in the local directory ``/var/nfs/datasets``.
      Update this to match the folder where you store your datasets.

      .. code-block:: yaml

         nfs-datasets:
           driver: local
           driver_opts:
             type: none
             o: bind
             device: /var/nfs/datasets

.. rubric:: Minio Username and Password

The environment variables ``MINIO_ROOT_USER`` and ``MINIO_ROOT_PASSWORD`` set the username and password needed to access the Testbed's S3 storage.
Currently, these variables are empty in the template, so fill them in (we recommend generating a long, random password for this purpose).
After you fill in these variables, you will also need to fill in all instances of the ``AWS_ACCESS_KEY_ID`` and ``AWS_SECRET_ACCESS_KEY`` variables with the same username and password.

.. rubric:: Configure the Testbed Workers

The sample deployment in ``docker-compose.yml`` has the following Testbed Workers:

- 2 Tensorflow CPU workers
- 3 Tensorflow GPU workers
- 2 PyTorch CPU workers
- 1 PyTorch GPU worker

Workers will likely need to be removed or added to suit your environment.
A good default is to have the same number of GPU workers as there are GPUs.
The number of CPU workers is more flexible and depends on how they're used.
Our default was one CPU worker for every 5 physical cores, but this can be scaled up or down to meet your needs.

Once you've settled on the number of CPU and GPU workers you need, the next step is to allocate the GPUs.
GPU-enabled containers must have the ``runtime: nvidia`` option set, and individual GPU devices are allocated using the ``NVIDIA_VISIBLE_DEVICES`` environment variable.
The GPUs are zero-indexed, so for example, if you have 2 GPUs, then the first one can be assigned using ``NVIDIA_VISIBLE_DEVICES: 0`` and the second one using ``NVIDIA_VISIBLE_DEVICES: 1``.

To allocate the CPUs, we use the ``cpuset`` and ``cpu_shares`` options.
Our deployment used the following:

.. code-block:: yaml

   services:
     # Block 1: CPUs 0 to 3
     redis:
       cpuset: 0-3
       cpu_shares: 1024
     minio:
       cpuset: 0-3
       cpu_shares: 1024
     mlflow-tracking:
       cpuset: 0-3
       cpu_shares: 1024
     nginx:
       cpuset: 0-3
       cpu_shares: 1024
     restpai:
       cpuset: 0-3
       cpu_shares: 1024

     # Block 2: CPUs 10 to 14
     tfcpu-01:
       cpuset: 10-14
       cpu_shares: 1024
     pytorchcpu-01:
       cpuset: 10-14
       cpu_shares: 1024

     # Block 3: CPUs 15 to 19
     tfcpu-02:
       cpuset: 15-19
       cpu_shares: 1024
     pytorchcpu-02:
       cpuset: 15-19
       cpu_shares: 1024

     # Block 4: CPUs 4 to 19
     tfgpu-01:
       cpuset: 4-19
       cpu_shares: 512
     tfgpu-02:
       cpuset: 4-19
       cpu_shares: 512
     tfgpu-03:
       cpuset: 4-19
       cpu_shares: 512
     pytorchgpu-01:
       cpuset: 4-19
       cpu_shares: 512

So, as we can see, in this arrangement the GPU Workers in Block 4 have exclusive access to CPUs 4 to 9, but also share CPUs 10 to 19 with Blocks 2 and 3.
The ``cpu_shares`` parameter controls the container's CPU priority when CPU resources are in high demand and only relative differences are meaningful.
So, for example, this means that the Workers in Block 2 have twice the CPU priority that the Workers in Block 4 have when using CPUS 10 to 14.

Tune the CPU and GPU allocations to suit your circumstances and needs.

Initializations
~~~~~~~~~~~~~~~

.. rubric:: Initialize the REST API database

The :term:`REST`  :term:`API` database must be initialized as follows before you start using it,

.. code-block:: sh

   # Initialize the database
   docker-compose run --rm restapi --upgrade-db

   # Teardown
   docker-compose down

.. rubric:: Create the S3 workflow bucket

The bucket used to cache the job submissions must be created before jobs can be submitted to the Testbed.

.. code-block:: sh

   # Create workflow bucket
   docker-compose run --rm --entrypoint "/bin/bash" restapi -c '/usr/local/bin/s3-mb.sh \
     --endpoint-url ${MLFLOW_S3_ENDPOINT_URL} workflow'

   # Teardown
   docker-compose down

.. rubric:: Configure and synchronize the task plugins

See the :doc:`Task Plugins Management guide <task-plugins-management>`.

Deployment
~~~~~~~~~~

With everything configured and initialized, you can deploy the Testbed.

.. code-block:: sh

   docker-compose up -d

To tear down the deployment, use

.. code-block:: sh

   docker-compose down
